{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Name: Arpit Aggarwal</b> <br>\n",
    "<b>UID: 116747189</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# header files\n",
    "import numpy as np\n",
    "import torch\n",
    "import h5py\n",
    "from matplotlib import pyplot as plt\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cat vs Non-Cat Image Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Loading Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using hw2.ipynb load_data() function. The load_data() function loads data from the training and testing files. Next step, is to flatten the image so that they can be fed as an input to the neural network. Lastly, the training and testing data is normalized between 0 and 1 which will be used for the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_input's shape: (209, 12288)\n",
      "test_input's shape: (50, 12288)\n"
     ]
    }
   ],
   "source": [
    "def load_data(train_file, test_file):\n",
    "    # Load the training data\n",
    "    train_dataset = h5py.File(train_file, 'r')\n",
    "    \n",
    "    # Separate features(x) and labels(y) for training set\n",
    "    train_set_x_orig = np.array(train_dataset['train_set_x'])\n",
    "    train_set_y_orig = np.array(train_dataset['train_set_y'])\n",
    "\n",
    "    # Load the test data\n",
    "    test_dataset = h5py.File(test_file, 'r')\n",
    "    \n",
    "    # Separate features(x) and labels(y) for training set\n",
    "    test_set_x_orig = np.array(test_dataset['test_set_x'])\n",
    "    test_set_y_orig = np.array(test_dataset['test_set_y'])\n",
    "    classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes\n",
    "    \n",
    "    train_set_y_orig = train_set_y_orig.reshape((train_set_y_orig.shape[0]))\n",
    "    test_set_y_orig = test_set_y_orig.reshape((test_set_y_orig.shape[0]))\n",
    "    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes\n",
    "\n",
    "# training and testing files\n",
    "train_file = \"data/train_catvnoncat.h5\"\n",
    "test_file = \"data/test_catvnoncat.h5\"\n",
    "train_x_orig, train_output, test_x_orig, test_output, classes = load_data(train_file, test_file) \n",
    "train_x_flatten = train_x_orig.reshape(train_x_orig.shape[0], -1)\n",
    "test_x_flatten = test_x_orig.reshape(test_x_orig.shape[0], -1)\n",
    "\n",
    "# Standardize data to have feature values between 0 and 1.\n",
    "train_input = train_x_flatten / 255.\n",
    "test_input = test_x_flatten / 255.\n",
    "\n",
    "# print data length\n",
    "print (\"train_input's shape: \" + str(train_input.shape))\n",
    "print (\"test_input's shape: \" + str(test_input.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Convert dataset to Tensor form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the dataset to Tensor form so that it can be fed into the PyTorch neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# use torch.from_numpy() to get the tensor form of the numpy array\n",
    "train_input = torch.from_numpy(train_input).float().to(device)\n",
    "train_output = torch.from_numpy(train_output).float().to(device)\n",
    "test_input = torch.from_numpy(test_input).float().to(device)\n",
    "test_output = torch.from_numpy(test_output).float().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Hyper-parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the hyper-parameters of the two-layer neural net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "num_epochs = 4000\n",
    "weight_decay = 0.001\n",
    "momentum = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Model-Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model-architecture is defined using pytorch Net class. The __init__ function is where we define the architecture of the neural network, i.e in this it is two layers. The forward function is where the forward pass step of the neural network takes place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=12288, out_features=7, bias=True)\n",
      "  (fc2): Linear(in_features=7, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# neural network class\n",
    "class Net(torch.nn.Module):\n",
    "    # init function\n",
    "    def __init__(self, num_input_neurons, num_hidden_neurons, num_output_neurons):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(num_input_neurons, num_hidden_neurons)\n",
    "        self.fc2 = torch.nn.Linear(num_hidden_neurons, num_output_neurons)\n",
    "        \n",
    "    # forward pass step of the neural network\n",
    "    def forward(self, input):\n",
    "        output = torch.nn.functional.sigmoid(self.fc2(torch.nn.functional.relu(self.fc1(input))))\n",
    "        return output\n",
    "\n",
    "# create object of the model\n",
    "net = Net(int(train_input.shape[1]), 7, 1).to(device)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use Binary Cross-entropy loss as we are doing image classification (cat vs non-cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "criterion = torch.nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is to define the optimizer we will be using for training the neural net. We will use gradient descent (full-batch) as out optimizer.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizers\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr = learning_rate, momentum = momentum, weight_decay = weight_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Training phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will be training the neural network to get the optimal set of weights and biases required for this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arpitdec5/.local/lib/python2.7/site-packages/torch/nn/functional.py:1351: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after iteration 100: 0.6271\n",
      "Loss after iteration 200: 0.5711\n",
      "Loss after iteration 300: 0.4925\n",
      "Loss after iteration 400: 0.4050\n",
      "Loss after iteration 500: 0.3196\n",
      "Loss after iteration 600: 0.2465\n",
      "Loss after iteration 700: 0.1891\n",
      "Loss after iteration 800: 0.1457\n",
      "Loss after iteration 900: 0.1137\n",
      "Loss after iteration 1000: 0.0903\n",
      "Loss after iteration 1100: 0.0731\n",
      "Loss after iteration 1200: 0.0604\n",
      "Loss after iteration 1300: 0.0507\n",
      "Loss after iteration 1400: 0.0432\n",
      "Loss after iteration 1500: 0.0373\n",
      "Loss after iteration 1600: 0.0326\n",
      "Loss after iteration 1700: 0.0288\n",
      "Loss after iteration 1800: 0.0256\n",
      "Loss after iteration 1900: 0.0230\n",
      "Loss after iteration 2000: 0.0208\n",
      "Loss after iteration 2100: 0.0189\n",
      "Loss after iteration 2200: 0.0173\n",
      "Loss after iteration 2300: 0.0160\n",
      "Loss after iteration 2400: 0.0148\n",
      "Loss after iteration 2500: 0.0137\n",
      "Loss after iteration 2600: 0.0128\n",
      "Loss after iteration 2700: 0.0120\n",
      "Loss after iteration 2800: 0.0113\n",
      "Loss after iteration 2900: 0.0106\n",
      "Loss after iteration 3000: 0.0100\n",
      "Loss after iteration 3100: 0.0095\n",
      "Loss after iteration 3200: 0.0090\n",
      "Loss after iteration 3300: 0.0086\n",
      "Loss after iteration 3400: 0.0082\n",
      "Loss after iteration 3500: 0.0078\n",
      "Loss after iteration 3600: 0.0075\n",
      "Loss after iteration 3700: 0.0072\n",
      "Loss after iteration 3800: 0.0069\n",
      "Loss after iteration 3900: 0.0067\n",
      "Loss after iteration 4000: 0.0064\n"
     ]
    }
   ],
   "source": [
    "# training phase\n",
    "for epoch in range(0, num_epochs):\n",
    "    \n",
    "    # forward step\n",
    "    pred_output = net(train_input)\n",
    "    \n",
    "    # find loss\n",
    "    loss = criterion(pred_output.squeeze(), train_output)\n",
    "    \n",
    "    # backpropagation step\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if((epoch + 1)%100 == 0):\n",
    "        print('Loss after iteration {}: {:.4f}' .format(epoch + 1, loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Testing Phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating model on testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy is: 74.0%\n"
     ]
    }
   ],
   "source": [
    "# testing phase\n",
    "net.eval()\n",
    "pred_output = net(test_input)\n",
    "loss = criterion(pred_output.squeeze(), test_output)\n",
    "#print(\"Testing Loss: \" + str(loss.item()))\n",
    "\n",
    "# accuracy\n",
    "correct = 0\n",
    "for index in range(0, len(pred_output)):\n",
    "    if(pred_output[index] > 0.5):\n",
    "        pred_output[index] = 1\n",
    "    else:\n",
    "        pred_output[index] = 0\n",
    "    \n",
    "    if(pred_output[index] == test_output[index]):\n",
    "        correct = correct + 1\n",
    "print(\"Testing accuracy is: \" + str(100.0 * float(float(correct) / len(pred_output))) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section contains all the hyper-parameters I tried and the corresponding accuracies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. learning_rate = 0.001, num_epochs = 4000, momentum = 0, weight_decay = 0, Testing Accuracy = 72%\n",
    "2. learning_rate = 0.005, num_epochs = 4000, momentum = 0, weight_decay = 0, Testing Accuracy = 66%\n",
    "3. learning_rate = 0.01, num_epochs = 4000, momentum = 0, weight_decay = 0, Testing Accuracy = 68%\n",
    "4. learning_rate = 0.05, num_epochs = 4000, momentum = 0, weight_decay = 0, Testing Accuracy = 64%\n",
    "5. learning_rate = 0.05, num_epochs = 4000, momentum = 0, weight_decay = 0.01, Testing Accuracy = 34%\n",
    "6. learning_rate = 0.01, num_epochs = 4000, momentum = 0, weight_decay = 0.01, Testing Accuracy = 74%\n",
    "7. learning_rate = 0.001, num_epochs = 4000, momentum = 0, weight_decay = 0.01, Testing Accuracy = 66%\n",
    "8. learning_rate = 0.005, num_epochs = 4000, momentum = 0, weight_decay = 0.01, Testing Accuracy = 70%\n",
    "9. learning_rate = 0.005, num_epochs = 4000, momentum = 0, weight_decay = 0.001, Testing Accuracy = 72%\n",
    "10. learning_rate = 0.001, num_epochs = 4000, momentum = 0, weight_decay = 0.001, Testing Accuracy = 64%\n",
    "11. learning_rate = 0.01, num_epochs = 4000, momentum = 0, weight_decay = 0.001, Testing Accuracy = 70%\n",
    "12. learning_rate = 0.05, num_epochs = 4000, momentum = 0, weight_decay = 0.001, Testing Accuracy = 64%\n",
    "13. learning_rate = 0.05, num_epochs = 4000, momentum = 0.9, weight_decay = 0.01, Testing Accuracy = 34%\n",
    "14. learning_rate = 0.01, num_epochs = 4000, momentum = 0.9, weight_decay = 0.01, Testing Accuracy = 58%\n",
    "15. learning_rate = 0.001, num_epochs = 4000, momentum = 0.9, weight_decay = 0.01, Testing Accuracy = 74%\n",
    "16. learning_rate = 0.005, num_epochs = 4000, momentum = 0.9, weight_decay = 0.01, Testing Accuracy = 72%\n",
    "17. learning_rate = 0.005, num_epochs = 4000, momentum = 0.9, weight_decay = 0.001, Testing Accuracy = 72%\n",
    "18. learning_rate = 0.001, num_epochs = 4000, momentum = 0.9, weight_decay = 0.001, Testing Accuracy = 74%\n",
    "19. learning_rate = 0.01, num_epochs = 4000, momentum = 0.9, weight_decay = 0.001, Testing Accuracy = 34%\n",
    "20. learning_rate = 0.05, num_epochs = 4000, momentum = 0.9, weight_decay = 0.001, Testing Accuracy = 34%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. Optimal Hyper-parameters obtained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal hyper-parameters obtained were as follows: <br><br>\n",
    "<b>learning_rate = 0.001, num_epochs = 4000, momentum = 0.9, weight_decay = 0.01, Testing Accuracy = 74%</b><br>\n",
    "<b>learning_rate = 0.01, num_epochs = 4000, momentum = 0, weight_decay = 0.01, Testing Accuracy = 74%</b><br>\n",
    "<b>learning_rate = 0.001, num_epochs = 4000, momentum = 0.9, weight_decay = 0.001, Testing Accuracy = 74%</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting sentiment of movie reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13. Loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the load_data function of hw2.ipynb and then preprocessing the data as done in the hw2.ipynb notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(train_file, test_file):\n",
    "    train_dataset = []\n",
    "    test_dataset = []\n",
    "    \n",
    "    # Read the training dataset file line by line\n",
    "    for line in open(train_file, 'r'):\n",
    "        train_dataset.append(line.strip())\n",
    "        \n",
    "    for line in open(test_file, 'r'):\n",
    "        test_dataset.append(line.strip())\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "def preprocess_reviews(reviews):\n",
    "    reviews = [REPLACE_NO_SPACE.sub(NO_SPACE, line.lower()) for line in reviews]\n",
    "    reviews = [REPLACE_WITH_SPACE.sub(SPACE, line) for line in reviews]\n",
    "    return reviews\n",
    "\n",
    "# loading data\n",
    "train_file = \"data/train_imdb.txt\"\n",
    "test_file = \"data/test_imdb.txt\"\n",
    "train_dataset, test_dataset = load_data(train_file, test_file)\n",
    "y = [1 if i < len(train_dataset)*0.5 else 0 for i in range(len(train_dataset))]\n",
    "\n",
    "# pre-processing\n",
    "REPLACE_NO_SPACE = re.compile(\"(\\.)|(\\;)|(\\:)|(\\!)|(\\')|(\\?)|(\\,)|(\\\")|(\\()|(\\))|(\\[)|(\\])|(\\d+)\")\n",
    "REPLACE_WITH_SPACE = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
    "NO_SPACE = \"\"\n",
    "SPACE = \" \"\n",
    "train_dataset_clean = preprocess_reviews(train_dataset)\n",
    "test_dataset_clean = preprocess_reviews(test_dataset)\n",
    "\n",
    "# Vectorization\n",
    "cv = CountVectorizer(binary=True, stop_words=\"english\", max_features=2000)\n",
    "cv.fit(train_dataset_clean)\n",
    "X = cv.transform(train_dataset_clean)\n",
    "X_test = cv.transform(test_dataset_clean)\n",
    "X = np.array(X.todense()).astype(float)\n",
    "X_test = np.array(X_test.todense()).astype(float)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14. Splitting of dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using sklearn for splitting dataset into training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arpitdec5/.local/lib/python2.7/site-packages/sklearn/model_selection/_split.py:2178: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, train_size = 0.80)\n",
    "y_train = y_train.reshape(1,-1)\n",
    "y_val = y_val.reshape(1,-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15. Convert dataset to Tensor form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the dataset to Tensor form so that it can be fed into the PyTorch neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# use torch.from_numpy() to get the tensor form of the numpy array\n",
    "train_input = torch.from_numpy(X_train).float().to(device)\n",
    "train_output = torch.from_numpy(y_train).float().to(device)\n",
    "train_output = train_output.squeeze()\n",
    "test_input = torch.from_numpy(X_val).float().to(device)\n",
    "test_output = torch.from_numpy(y_val).float().to(device)\n",
    "test_output = test_output.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16. Hyper-parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the hyper-parameters for the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.005\n",
    "num_epochs = 4000\n",
    "momentum = 0.9\n",
    "weight_decay = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 17. Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model-architecture is defined using pytorch Net class. The __init__ function is where we define the architecture of the neural network, i.e in this it is two layers. The forward function is where the forward pass step of the neural network takes place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=2000, out_features=200, bias=True)\n",
      "  (fc2): Linear(in_features=200, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# neural network class\n",
    "class Net(torch.nn.Module):\n",
    "    # init function\n",
    "    def __init__(self, num_input_neurons, num_hidden_neurons, num_output_neurons):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(num_input_neurons, num_hidden_neurons)\n",
    "        self.fc2 = torch.nn.Linear(num_hidden_neurons, num_output_neurons)\n",
    "        \n",
    "    # forward pass step of the neural network\n",
    "    def forward(self, input):\n",
    "        output = torch.nn.functional.sigmoid(self.fc2(torch.nn.functional.relu(self.fc1(input))))\n",
    "        return output\n",
    "    \n",
    "# get the neural net object\n",
    "net = Net(int(train_input.shape[1]), 200, 1).to(device)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 18. Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use Binary Cross-entropy loss as we are doing sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "criterion = torch.nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 19. Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is to define the optimizer we will be using for training the neural net. We will use gradient descent (full-batch) as out optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr = learning_rate, momentum = momentum, weight_decay = weight_decay) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20. Training phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will be training the neural network to get the optimal set of weights and biases required for this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after iteration 100: 0.6592\n",
      "Loss after iteration 200: 0.5364\n",
      "Loss after iteration 300: 0.3436\n",
      "Loss after iteration 400: 0.2252\n",
      "Loss after iteration 500: 0.1608\n",
      "Loss after iteration 600: 0.1229\n",
      "Loss after iteration 700: 0.0991\n",
      "Loss after iteration 800: 0.0834\n",
      "Loss after iteration 900: 0.0727\n",
      "Loss after iteration 1000: 0.0652\n",
      "Loss after iteration 1100: 0.0598\n",
      "Loss after iteration 1200: 0.0558\n",
      "Loss after iteration 1300: 0.0529\n",
      "Loss after iteration 1400: 0.0507\n",
      "Loss after iteration 1500: 0.0489\n",
      "Loss after iteration 1600: 0.0476\n",
      "Loss after iteration 1700: 0.0466\n",
      "Loss after iteration 1800: 0.0458\n",
      "Loss after iteration 1900: 0.0451\n",
      "Loss after iteration 2000: 0.0446\n",
      "Loss after iteration 2100: 0.0442\n",
      "Loss after iteration 2200: 0.0438\n",
      "Loss after iteration 2300: 0.0435\n",
      "Loss after iteration 2400: 0.0433\n",
      "Loss after iteration 2500: 0.0431\n",
      "Loss after iteration 2600: 0.0429\n",
      "Loss after iteration 2700: 0.0428\n",
      "Loss after iteration 2800: 0.0427\n",
      "Loss after iteration 2900: 0.0426\n",
      "Loss after iteration 3000: 0.0425\n",
      "Loss after iteration 3100: 0.0424\n",
      "Loss after iteration 3200: 0.0423\n",
      "Loss after iteration 3300: 0.0423\n",
      "Loss after iteration 3400: 0.0422\n",
      "Loss after iteration 3500: 0.0422\n",
      "Loss after iteration 3600: 0.0421\n",
      "Loss after iteration 3700: 0.0421\n",
      "Loss after iteration 3800: 0.0421\n",
      "Loss after iteration 3900: 0.0421\n",
      "Loss after iteration 4000: 0.0420\n"
     ]
    }
   ],
   "source": [
    "# training phase\n",
    "for epoch in range(0, num_epochs):\n",
    "    \n",
    "    # forward step\n",
    "    pred_output = net(train_input)\n",
    "    \n",
    "    # find loss\n",
    "    loss = criterion(pred_output.squeeze(), train_output)\n",
    "    \n",
    "    # backpropagation step\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if((epoch + 1)%100 == 0):\n",
    "        print('Loss after iteration {}: {:.4f}' .format(epoch + 1, loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 21. Testing phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating model on testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy is: 87.5621890547%\n"
     ]
    }
   ],
   "source": [
    "# testing phase\n",
    "net.eval()\n",
    "pred_output = net(test_input)\n",
    "loss = criterion(pred_output.squeeze(), test_output)\n",
    "\n",
    "# accuracy\n",
    "correct = 0\n",
    "for index in range(0, len(pred_output)):\n",
    "    if(pred_output[index] > 0.5):\n",
    "        pred_output[index] = 1\n",
    "    else:\n",
    "        pred_output[index] = 0\n",
    "    \n",
    "    if(pred_output[index] == test_output[index]):\n",
    "        correct = correct + 1\n",
    "print(\"Testing accuracy is: \" + str(100.0 * float(float(correct) / len(pred_output))) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 22. Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section contains all the hyper-parameters I tried and the corresponding accuracies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. learning_rate = 0.05, num_epochs = 4000, momentum = 0, weight_decay = 0, Testing Accuracy = 89.05%\n",
    "2. learning_rate = 0.01, num_epochs = 4000, momentum = 0, weight_decay = 0, Testing Accuracy = 89.05%\n",
    "3. learning_rate = 0.001, num_epochs = 4000, momentum = 0, weight_decay = 0, Testing Accuracy = 77.11%\n",
    "4. learning_rate = 0.005, num_epochs = 4000, momentum = 0, weight_decay = 0, Testing Accuracy = 89.05%\n",
    "5. learning_rate = 0.05, num_epochs = 4000, momentum = 0, weight_decay = 0.01, Testing Accuracy = 89.55%\n",
    "6. learning_rate = 0.01, num_epochs = 4000, momentum = 0, weight_decay = 0.01, Testing Accuracy = 89.55%\n",
    "7. learning_rate = 0.001, num_epochs = 4000, momentum = 0, weight_decay = 0.01, Testing Accuracy = 74.62%\n",
    "8. learning_rate = 0.005, num_epochs = 4000, momentum = 0, weight_decay = 0.01, Testing Accuracy = 90.04%\n",
    "9. learning_rate = 0.05, num_epochs = 4000, momentum = 0, weight_decay = 0.001, Testing Accuracy = 89.05%\n",
    "10. learning_rate = 0.01, num_epochs = 4000, momentum = 0, weight_decay = 0.001, Testing Accuracy = 89.05%\n",
    "11. learning_rate = 0.001, num_epochs = 4000, momentum = 0, weight_decay = 0.001, Testing Accuracy = 68.65%\n",
    "12. learning_rate = 0.005, num_epochs = 4000, momentum = 0, weight_decay = 0.001, Testing Accuracy = 90.04%\n",
    "13. learning_rate = 0.05, num_epochs = 4000, momentum = 0.9, weight_decay = 0.001, Testing Accuracy = 89.05%\n",
    "14. learning_rate = 0.01, num_epochs = 4000, momentum = 0.9, weight_decay = 0.001, Testing Accuracy = 89.05%\n",
    "15. learning_rate = 0.001, num_epochs = 4000, momentum = 0.9, weight_decay = 0.001, Testing Accuracy = 89.55%\n",
    "16. learning_rate = 0.005, num_epochs = 4000, momentum = 0.9, weight_decay = 0.001, Testing Accuracy = 89.045%\n",
    "17. learning_rate = 0.05, num_epochs = 4000, momentum = 0.9, weight_decay = 0.01, Testing Accuracy = 90.04%\n",
    "18. learning_rate = 0.01, num_epochs = 4000, momentum = 0.9, weight_decay = 0.01, Testing Accuracy = 90.04%\n",
    "19. learning_rate = 0.001, num_epochs = 4000, momentum = 0.9, weight_decay = 0.01, Testing Accuracy = 89.55%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 23. Optimal Hyper-parameters obtained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal hyper-parameters obtained were as follows: <br><br>\n",
    "<b>learning_rate = 0.005, num_epochs = 4000, momentum = 0, weight_decay = 0.01, Testing Accuracy = 90.04%</b><br>\n",
    "<b>learning_rate = 0.005, num_epochs = 4000, momentum = 0, weight_decay = 0.001, Testing Accuracy = 90.04%</b><br>\n",
    "<b>learning_rate = 0.05, num_epochs = 4000, momentum = 0.9, weight_decay = 0.01, Testing Accuracy = 90.04%</b><br>\n",
    "<b>learning_rate = 0.01, num_epochs = 4000, momentum = 0.9, weight_decay = 0.01, Testing Accuracy = 90.04%</b><br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
