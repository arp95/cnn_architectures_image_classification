{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Name: Arpit Aggarwal</b> <br>\n",
    "<b>UID: 116747189</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# header files\n",
    "import numpy as np\n",
    "import torch\n",
    "import h5py\n",
    "from matplotlib import pyplot as plt\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data.sampler import SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cat vs Non-Cat Image Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Loading Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using hw2.ipynb load_data() function. The load_data() function loads data from the training and testing files. Next step, is to flatten the image so that they can be fed as an input to the neural network. Lastly, the training and testing data is normalized between 0 and 1 which will be used for the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_input's shape: (209, 12288)\n",
      "test_input's shape: (50, 12288)\n"
     ]
    }
   ],
   "source": [
    "def load_data(train_file, test_file):\n",
    "    # Load the training data\n",
    "    train_dataset = h5py.File(train_file, 'r')\n",
    "    \n",
    "    # Separate features(x) and labels(y) for training set\n",
    "    train_set_x_orig = np.array(train_dataset['train_set_x'])\n",
    "    train_set_y_orig = np.array(train_dataset['train_set_y'])\n",
    "\n",
    "    # Load the test data\n",
    "    test_dataset = h5py.File(test_file, 'r')\n",
    "    \n",
    "    # Separate features(x) and labels(y) for training set\n",
    "    test_set_x_orig = np.array(test_dataset['test_set_x'])\n",
    "    test_set_y_orig = np.array(test_dataset['test_set_y'])\n",
    "    classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes\n",
    "    \n",
    "    train_set_y_orig = train_set_y_orig.reshape((train_set_y_orig.shape[0]))\n",
    "    test_set_y_orig = test_set_y_orig.reshape((test_set_y_orig.shape[0]))\n",
    "    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes\n",
    "\n",
    "# training and testing files\n",
    "train_file = \"data/train_catvnoncat.h5\"\n",
    "test_file = \"data/test_catvnoncat.h5\"\n",
    "train_x_orig, train_output, test_x_orig, test_output, classes = load_data(train_file, test_file) \n",
    "train_x_flatten = train_x_orig.reshape(train_x_orig.shape[0], -1)\n",
    "test_x_flatten = test_x_orig.reshape(test_x_orig.shape[0], -1)\n",
    "\n",
    "# Standardize data to have feature values between 0 and 1.\n",
    "train_input = train_x_flatten / 255.\n",
    "test_input = test_x_flatten / 255.\n",
    "\n",
    "# print data length\n",
    "print (\"train_input's shape: \" + str(train_input.shape))\n",
    "print (\"test_input's shape: \" + str(test_input.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Convert dataset to Tensor form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the dataset to Tensor form so that it can be fed into the PyTorch neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# use torch.from_numpy() to get the tensor form of the numpy array\n",
    "train_input = torch.from_numpy(train_input).float().to(device)\n",
    "test_input = torch.from_numpy(test_input).float().to(device)\n",
    "train_output = torch.from_numpy(train_output).float().to(device)\n",
    "test_output = torch.from_numpy(test_output).float().to(device)\n",
    "valid_input = train_input[169:]\n",
    "train_input = train_input[:169]\n",
    "valid_output = train_output[169:]\n",
    "train_output = train_output[:169]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Hyper-parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the hyper-parameters of the two-layer neural net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.005\n",
    "num_epochs = 1000\n",
    "weight_decay = 0.001\n",
    "momentum = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Model-Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model-architecture is defined using pytorch Net class. The __init__ function is where we define the architecture of the neural network, i.e in this it is two layers. The forward function is where the forward pass step of the neural network takes place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=12288, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (fc3): Linear(in_features=512, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# neural network class\n",
    "class Net(torch.nn.Module):\n",
    "    # init function\n",
    "    def __init__(self, num_input_neurons, num_hidden_neurons_1, num_hidden_neurons_2, num_output_neurons):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(num_input_neurons, num_hidden_neurons_1)\n",
    "        self.fc2 = torch.nn.Linear(num_hidden_neurons_1, num_hidden_neurons_2)\n",
    "        self.fc3 = torch.nn.Linear(num_hidden_neurons_2, num_output_neurons)\n",
    "        self.dropout = torch.nn.Dropout(0.2)\n",
    "        \n",
    "    # forward pass step of the neural network\n",
    "    def forward(self, input):\n",
    "        x = torch.nn.functional.relu(self.fc1(input))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.nn.functional.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        output = torch.nn.functional.sigmoid(self.fc3(x))\n",
    "        return output\n",
    "\n",
    "# create object of the model\n",
    "net = Net(int(train_input.shape[1]), 512, 512, 1).to(device)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use Binary Cross-entropy loss as we are doing image classification (cat vs non-cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "criterion = torch.nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is to define the optimizer we will be using for training the neural net. We will use gradient descent (full-batch) as out optimizer.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizers\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr = learning_rate, momentum = momentum, weight_decay = weight_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Training phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will be training the neural network to get the optimal set of weights and biases required for this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3391, Valid Loss: 0.6948\n",
      "Train Loss: 0.0288, Valid Loss: 1.3186\n",
      "Train Loss: 0.0087, Valid Loss: 1.6460\n",
      "Train Loss: 0.0047, Valid Loss: 1.8148\n",
      "Train Loss: 0.0032, Valid Loss: 1.9228\n",
      "Train Loss: 0.0024, Valid Loss: 1.9998\n",
      "Train Loss: 0.0019, Valid Loss: 2.0574\n",
      "Train Loss: 0.0016, Valid Loss: 2.1020\n",
      "Train Loss: 0.0014, Valid Loss: 2.1377\n",
      "Train Loss: 0.0012, Valid Loss: 2.1668\n"
     ]
    }
   ],
   "source": [
    "# training phase\n",
    "for epoch in range(0, num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "        \n",
    "    # forward step\n",
    "    pred_output = net(train_input)\n",
    "        \n",
    "    # find loss\n",
    "    loss = criterion(pred_output.squeeze(), train_output)\n",
    "    \n",
    "    # backpropagation step\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "        \n",
    "    # update train loss\n",
    "    train_loss = loss.item()\n",
    "    \n",
    "    net.eval()\n",
    "    pred_output = net(valid_input)\n",
    "        \n",
    "    # find loss\n",
    "    loss = criterion(pred_output.squeeze(), valid_output)\n",
    "    \n",
    "    # update train loss\n",
    "    valid_loss = loss.item()\n",
    "    \n",
    "    if((epoch + 1)%100 == 0):\n",
    "        print('Train Loss: {:.4f}, Valid Loss: {:.4f}' .format(train_loss, valid_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Testing Phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating model on testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy is: 76.0%\n"
     ]
    }
   ],
   "source": [
    "# testing phase\n",
    "net.eval()\n",
    "pred_output = net(test_input)\n",
    "loss = criterion(pred_output.squeeze(), test_output)\n",
    "#print(\"Testing Loss: \" + str(loss.item()))\n",
    "\n",
    "# accuracy\n",
    "correct = 0\n",
    "for index in range(0, len(pred_output)):\n",
    "    if(pred_output[index] > 0.5):\n",
    "        pred_output[index] = 1\n",
    "    else:\n",
    "        pred_output[index] = 0\n",
    "    \n",
    "    if(pred_output[index] == test_output[index]):\n",
    "        correct = correct + 1\n",
    "print(\"Testing accuracy is: \" + str(100.0 * float(float(correct) / len(pred_output))) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section contains all the hyper-parameters I tried and the corresponding accuracies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. learning_rate = 0.001, num_epochs = 4000, momentum = 0, weight_decay = 0, Testing Accuracy = 72%\n",
    "2. learning_rate = 0.005, num_epochs = 4000, momentum = 0, weight_decay = 0, Testing Accuracy = 66%\n",
    "3. learning_rate = 0.01, num_epochs = 4000, momentum = 0, weight_decay = 0, Testing Accuracy = 68%\n",
    "4. learning_rate = 0.05, num_epochs = 4000, momentum = 0, weight_decay = 0, Testing Accuracy = 64%\n",
    "5. learning_rate = 0.05, num_epochs = 4000, momentum = 0, weight_decay = 0.01, Testing Accuracy = 34%\n",
    "6. learning_rate = 0.01, num_epochs = 4000, momentum = 0, weight_decay = 0.01, Testing Accuracy = 74%\n",
    "7. learning_rate = 0.001, num_epochs = 4000, momentum = 0, weight_decay = 0.01, Testing Accuracy = 66%\n",
    "8. learning_rate = 0.005, num_epochs = 4000, momentum = 0, weight_decay = 0.01, Testing Accuracy = 70%\n",
    "9. learning_rate = 0.005, num_epochs = 4000, momentum = 0, weight_decay = 0.001, Testing Accuracy = 72%\n",
    "10. learning_rate = 0.001, num_epochs = 4000, momentum = 0, weight_decay = 0.001, Testing Accuracy = 64%\n",
    "11. learning_rate = 0.01, num_epochs = 4000, momentum = 0, weight_decay = 0.001, Testing Accuracy = 70%\n",
    "12. learning_rate = 0.05, num_epochs = 4000, momentum = 0, weight_decay = 0.001, Testing Accuracy = 64%\n",
    "13. learning_rate = 0.05, num_epochs = 4000, momentum = 0.9, weight_decay = 0.01, Testing Accuracy = 34%\n",
    "14. learning_rate = 0.01, num_epochs = 4000, momentum = 0.9, weight_decay = 0.01, Testing Accuracy = 58%\n",
    "15. learning_rate = 0.001, num_epochs = 4000, momentum = 0.9, weight_decay = 0.01, Testing Accuracy = 74%\n",
    "16. learning_rate = 0.005, num_epochs = 4000, momentum = 0.9, weight_decay = 0.01, Testing Accuracy = 72%\n",
    "17. learning_rate = 0.005, num_epochs = 4000, momentum = 0.9, weight_decay = 0.001, Testing Accuracy = 72%\n",
    "18. learning_rate = 0.001, num_epochs = 4000, momentum = 0.9, weight_decay = 0.001, Testing Accuracy = 74%\n",
    "19. learning_rate = 0.01, num_epochs = 4000, momentum = 0.9, weight_decay = 0.001, Testing Accuracy = 34%\n",
    "20. learning_rate = 0.05, num_epochs = 4000, momentum = 0.9, weight_decay = 0.001, Testing Accuracy = 34%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. Optimal Hyper-parameters obtained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal hyper-parameters obtained were as follows: <br><br>\n",
    "<b>learning_rate = 0.001, num_epochs = 4000, momentum = 0.9, weight_decay = 0.01, Testing Accuracy = 74%</b><br>\n",
    "<b>learning_rate = 0.01, num_epochs = 4000, momentum = 0, weight_decay = 0.01, Testing Accuracy = 74%</b><br>\n",
    "<b>learning_rate = 0.001, num_epochs = 4000, momentum = 0.9, weight_decay = 0.001, Testing Accuracy = 74%</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting sentiment of movie reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13. Loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the load_data function of hw2.ipynb and then preprocessing the data as done in the hw2.ipynb notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(train_file, test_file):\n",
    "    train_dataset = []\n",
    "    test_dataset = []\n",
    "    \n",
    "    # Read the training dataset file line by line\n",
    "    for line in open(train_file, 'r'):\n",
    "        train_dataset.append(line.strip())\n",
    "        \n",
    "    for line in open(test_file, 'r'):\n",
    "        test_dataset.append(line.strip())\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "def preprocess_reviews(reviews):\n",
    "    reviews = [REPLACE_NO_SPACE.sub(NO_SPACE, line.lower()) for line in reviews]\n",
    "    reviews = [REPLACE_WITH_SPACE.sub(SPACE, line) for line in reviews]\n",
    "    return reviews\n",
    "\n",
    "# loading data\n",
    "train_file = \"data/train_imdb.txt\"\n",
    "test_file = \"data/test_imdb.txt\"\n",
    "train_dataset, test_dataset = load_data(train_file, test_file)\n",
    "y = [1 if i < len(train_dataset)*0.5 else 0 for i in range(len(train_dataset))]\n",
    "\n",
    "# pre-processing\n",
    "REPLACE_NO_SPACE = re.compile(\"(\\.)|(\\;)|(\\:)|(\\!)|(\\')|(\\?)|(\\,)|(\\\")|(\\()|(\\))|(\\[)|(\\])|(\\d+)\")\n",
    "REPLACE_WITH_SPACE = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
    "NO_SPACE = \"\"\n",
    "SPACE = \" \"\n",
    "train_dataset_clean = preprocess_reviews(train_dataset)\n",
    "test_dataset_clean = preprocess_reviews(test_dataset)\n",
    "\n",
    "# Vectorization\n",
    "cv = CountVectorizer(binary=True, stop_words=\"english\", max_features=2000)\n",
    "cv.fit(train_dataset_clean)\n",
    "X = cv.transform(train_dataset_clean)\n",
    "X_test = cv.transform(test_dataset_clean)\n",
    "X = np.array(X.todense()).astype(float)\n",
    "X_test = np.array(X_test.todense()).astype(float)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14. Splitting of dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using sklearn for splitting dataset into training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, train_size = 0.80)\n",
    "y_train = y_train.reshape(1,-1)\n",
    "y_val = y_val.reshape(1,-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15. Convert dataset to Tensor form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the dataset to Tensor form so that it can be fed into the PyTorch neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# use torch.from_numpy() to get the tensor form of the numpy array\n",
    "train_input = torch.from_numpy(X_train).float().to(device)\n",
    "train_output = torch.from_numpy(y_train).float().to(device)\n",
    "train_output = train_output.squeeze()\n",
    "test_input = torch.from_numpy(X_val).float().to(device)\n",
    "test_output = torch.from_numpy(y_val).float().to(device)\n",
    "test_output = test_output.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16. Hyper-parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the hyper-parameters for the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.005\n",
    "num_epochs = 4000\n",
    "momentum = 0.9\n",
    "weight_decay = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 17. Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model-architecture is defined using pytorch Net class. The __init__ function is where we define the architecture of the neural network, i.e in this it is two layers. The forward function is where the forward pass step of the neural network takes place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural network class\n",
    "class Net(torch.nn.Module):\n",
    "    # init function\n",
    "    def __init__(self, num_input_neurons, num_hidden_neurons, num_output_neurons):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(num_input_neurons, num_hidden_neurons)\n",
    "        self.fc2 = torch.nn.Linear(num_hidden_neurons, num_output_neurons)\n",
    "        \n",
    "    # forward pass step of the neural network\n",
    "    def forward(self, input):\n",
    "        output = torch.nn.functional.sigmoid(self.fc2(torch.nn.functional.relu(self.fc1(input))))\n",
    "        return output\n",
    "    \n",
    "# get the neural net object\n",
    "net = Net(int(train_input.shape[1]), 200, 1).to(device)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 18. Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use Binary Cross-entropy loss as we are doing sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "criterion = torch.nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 19. Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is to define the optimizer we will be using for training the neural net. We will use gradient descent (full-batch) as out optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr = learning_rate, momentum = momentum, weight_decay = weight_decay) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20. Training phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will be training the neural network to get the optimal set of weights and biases required for this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training phase\n",
    "for epoch in range(0, num_epochs):\n",
    "    \n",
    "    # forward step\n",
    "    pred_output = net(train_input)\n",
    "    \n",
    "    # find loss\n",
    "    loss = criterion(pred_output.squeeze(), train_output)\n",
    "    \n",
    "    # backpropagation step\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if((epoch + 1)%100 == 0):\n",
    "        print('Loss after iteration {}: {:.4f}' .format(epoch + 1, loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 21. Testing phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating model on testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing phase\n",
    "net.eval()\n",
    "pred_output = net(test_input)\n",
    "loss = criterion(pred_output.squeeze(), test_output)\n",
    "\n",
    "# accuracy\n",
    "correct = 0\n",
    "for index in range(0, len(pred_output)):\n",
    "    if(pred_output[index] > 0.5):\n",
    "        pred_output[index] = 1\n",
    "    else:\n",
    "        pred_output[index] = 0\n",
    "    \n",
    "    if(pred_output[index] == test_output[index]):\n",
    "        correct = correct + 1\n",
    "print(\"Testing accuracy is: \" + str(100.0 * float(float(correct) / len(pred_output))) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 22. Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section contains all the hyper-parameters I tried and the corresponding accuracies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. learning_rate = 0.05, num_epochs = 4000, momentum = 0, weight_decay = 0, Testing Accuracy = 89.05%\n",
    "2. learning_rate = 0.01, num_epochs = 4000, momentum = 0, weight_decay = 0, Testing Accuracy = 89.05%\n",
    "3. learning_rate = 0.001, num_epochs = 4000, momentum = 0, weight_decay = 0, Testing Accuracy = 77.11%\n",
    "4. learning_rate = 0.005, num_epochs = 4000, momentum = 0, weight_decay = 0, Testing Accuracy = 89.05%\n",
    "5. learning_rate = 0.05, num_epochs = 4000, momentum = 0, weight_decay = 0.01, Testing Accuracy = 89.55%\n",
    "6. learning_rate = 0.01, num_epochs = 4000, momentum = 0, weight_decay = 0.01, Testing Accuracy = 89.55%\n",
    "7. learning_rate = 0.001, num_epochs = 4000, momentum = 0, weight_decay = 0.01, Testing Accuracy = 74.62%\n",
    "8. learning_rate = 0.005, num_epochs = 4000, momentum = 0, weight_decay = 0.01, Testing Accuracy = 90.04%\n",
    "9. learning_rate = 0.05, num_epochs = 4000, momentum = 0, weight_decay = 0.001, Testing Accuracy = 89.05%\n",
    "10. learning_rate = 0.01, num_epochs = 4000, momentum = 0, weight_decay = 0.001, Testing Accuracy = 89.05%\n",
    "11. learning_rate = 0.001, num_epochs = 4000, momentum = 0, weight_decay = 0.001, Testing Accuracy = 68.65%\n",
    "12. learning_rate = 0.005, num_epochs = 4000, momentum = 0, weight_decay = 0.001, Testing Accuracy = 90.04%\n",
    "13. learning_rate = 0.05, num_epochs = 4000, momentum = 0.9, weight_decay = 0.001, Testing Accuracy = 89.05%\n",
    "14. learning_rate = 0.01, num_epochs = 4000, momentum = 0.9, weight_decay = 0.001, Testing Accuracy = 89.05%\n",
    "15. learning_rate = 0.001, num_epochs = 4000, momentum = 0.9, weight_decay = 0.001, Testing Accuracy = 89.55%\n",
    "16. learning_rate = 0.005, num_epochs = 4000, momentum = 0.9, weight_decay = 0.001, Testing Accuracy = 89.045%\n",
    "17. learning_rate = 0.05, num_epochs = 4000, momentum = 0.9, weight_decay = 0.01, Testing Accuracy = 90.04%\n",
    "18. learning_rate = 0.01, num_epochs = 4000, momentum = 0.9, weight_decay = 0.01, Testing Accuracy = 90.04%\n",
    "19. learning_rate = 0.001, num_epochs = 4000, momentum = 0.9, weight_decay = 0.01, Testing Accuracy = 89.55%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 23. Optimal Hyper-parameters obtained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal hyper-parameters obtained were as follows: <br><br>\n",
    "<b>learning_rate = 0.005, num_epochs = 4000, momentum = 0, weight_decay = 0.01, Testing Accuracy = 90.04%</b><br>\n",
    "<b>learning_rate = 0.005, num_epochs = 4000, momentum = 0, weight_decay = 0.001, Testing Accuracy = 90.04%</b><br>\n",
    "<b>learning_rate = 0.05, num_epochs = 4000, momentum = 0.9, weight_decay = 0.01, Testing Accuracy = 90.04%</b><br>\n",
    "<b>learning_rate = 0.01, num_epochs = 4000, momentum = 0.9, weight_decay = 0.01, Testing Accuracy = 90.04%</b><br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
